#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¿»è¯‘ç¬¬äºŒç¯‡æ–‡ç« çš„æ­£æ–‡æ®µè½
Advanced Amazon Data Extraction Best Practices
"""

import re

filename = 'zh/articles/advanced-amazon-data-extraction-best-practices.html'
print(f"ğŸ“ æ­£åœ¨ç¿»è¯‘æ­£æ–‡æ®µè½: {filename}")

with open(filename, 'r', encoding='utf-8') as f:
    content = f.read()

translations = [
    # Intro
    (
        r'While basic product scraping is straightforward, extracting data at an enterprise scale introduces a new set of complexities\. Rate limits, IP blocks, data validation, and concurrent request management become critical factors in your success\.',
        'è™½ç„¶åŸºç¡€çš„äº§å“æŠ“å–å¾ˆç®€å•ï¼Œä½†åœ¨ä¼ä¸šçº§è§„æ¨¡ä¸Šæå–æ•°æ®ä¼šå¼•å…¥ä¸€ç³»åˆ—æ–°çš„å¤æ‚æ€§ã€‚é€Ÿç‡é™åˆ¶ã€IP å°ç¦ã€æ•°æ®éªŒè¯å’Œå¹¶å‘è¯·æ±‚ç®¡ç†æˆä¸ºæˆåŠŸçš„å…³é”®å› ç´ ã€‚'
    ),
    (
        r'In this advanced guide, we\'ll explore the best practices and optimization strategies used by top e-commerce data teams to build robust, scalable Amazon scraping pipelines using Pangol Info\'s API\.',
        'åœ¨è¿™ä»½é«˜çº§æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨é¡¶çº§ç”µå•†æ•°æ®å›¢é˜Ÿä½¿ç”¨çš„æœ€ä½³å®è·µå’Œä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨ Pangol Info çš„ API æ„å»ºç¨³å¥ã€å¯æ‰©å±•çš„ Amazon æŠ“å–ç®¡é“ã€‚'
    ),
    # Understanding Architecture
    (
        r'Before optimizing, it\'s crucial to understand how modern Amazon APIs handle requests\. A robust architecture typically consists of:',
        'åœ¨ä¼˜åŒ–ä¹‹å‰ï¼Œäº†è§£ç°ä»£ Amazon API å¦‚ä½•å¤„ç†è¯·æ±‚è‡³å…³é‡è¦ã€‚ä¸€ä¸ªç¨³å¥çš„æ¶æ„é€šå¸¸åŒ…æ‹¬ï¼š'
    ),
    (
        r'<strong>Request Scheduler:</strong> Manages the queue and timing of outgoing API calls',
        '<strong>è¯·æ±‚è°ƒåº¦å™¨ï¼š</strong> ç®¡ç†ä¼ å‡º API è°ƒç”¨çš„é˜Ÿåˆ—å’Œæ—¶é—´'
    ),
    (
        r'<strong>Proxy & Rotation Layer:</strong> Handled automatically by Pangol Info, but meaningful to understand',
        '<strong>ä»£ç†å’Œè½®æ¢å±‚ï¼š</strong> ç”± Pangol Info è‡ªåŠ¨å¤„ç†ï¼Œä½†ç†è§£å…¶åŸç†å¾ˆæœ‰æ„ä¹‰'
    ),
    (
        r'<strong>Parser & Validator:</strong> Checks response integrity before storage',
        '<strong>è§£æå™¨å’ŒéªŒè¯å™¨ï¼š</strong> åœ¨å­˜å‚¨ä¹‹å‰æ£€æŸ¥å“åº”çš„å®Œæ•´æ€§'
    ),
    (
        r'<strong>Storage Layer:</strong> Efficiently saves structured data',
        '<strong>å­˜å‚¨å±‚ï¼š</strong> é«˜æ•ˆä¿å­˜ç»“æ„åŒ–æ•°æ®'
    ),
    # Advanced Extraction Techniques
    (
        r'When dealing with thousands of ASINs, sequential processing is too slow\. Implementing parallel processing with proper concurrency control is essential\.',
        'åœ¨å¤„ç†æ•°åƒä¸ª ASIN æ—¶ï¼Œé¡ºåºå¤„ç†å¤ªæ…¢äº†ã€‚å®æ–½å…·æœ‰é€‚å½“å¹¶å‘æ§åˆ¶çš„å¹¶è¡Œå¤„ç†è‡³å…³é‡è¦ã€‚'
    ),
    (
        r'Here is an example using Python\'s <code[^>]*>concurrent\.futures</code>:',
        'è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ Python <code class="text-accent-cyan">concurrent.futures</code> çš„ç¤ºä¾‹ï¼š'
    ),
    # Batch Processing
    (
        r'Instead of processing items one by one, group them into batches\. This reduces overhead and makes error recovery easier\. If a batch fails, you only need to retry that specific subset of ASINs\.',
        'ä¸è¦é€ä¸ªå¤„ç†é¡¹ç›®ï¼Œè€Œæ˜¯å°†å®ƒä»¬åˆ†ç»„ä¸ºæ‰¹æ¬¡ã€‚è¿™å‡å°‘äº†å¼€é”€å¹¶ä½¿é”™è¯¯æ¢å¤æ›´å®¹æ˜“ã€‚å¦‚æœä¸€ä¸ªæ‰¹æ¬¡å¤±è´¥ï¼Œæ‚¨åªéœ€è¦é‡è¯•è¯¥ç‰¹å®šçš„ ASIN å­é›†ã€‚'
    ),
    # Data Validation
    (
        r'Never trust the incoming data blindly\. Amazon\'s HTML structure changes frequently, which can lead to parsing errors\. Implement a validation layer to ensure data quality\.',
        'æ°¸è¿œä¸è¦ç›²ç›®ä¿¡ä»»ä¼ å…¥çš„æ•°æ®ã€‚Amazon çš„ HTML ç»“æ„é¢‘ç¹å˜åŒ–ï¼Œå¯èƒ½å¯¼è‡´è§£æé”™è¯¯ã€‚å®æ–½éªŒè¯å±‚ä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚'
    ),
    (
        r'Key checks to implement:',
        'éœ€è¦å®æ–½çš„å…³é”®æ£€æŸ¥ï¼š'
    ),
    (
        r'<strong>Required Fields:</strong> Ensure critical fields like price and title are present',
        '<strong>å¿…å¡«å­—æ®µï¼š</strong> ç¡®ä¿å­˜åœ¨ä»·æ ¼å’Œæ ‡é¢˜ç­‰å…³é”®å­—æ®µ'
    ),
    (
        r'<strong>Data Types:</strong> Verify price is a number, date is valid, etc\.',
        '<strong>æ•°æ®ç±»å‹ï¼š</strong> éªŒè¯ä»·æ ¼æ˜¯æ•°å­—ï¼Œæ—¥æœŸæœ‰æ•ˆç­‰ã€‚'
    ),
    (
        r'<strong>Logic Checks:</strong> Price shouldn\'t be zero unless free',
        '<strong>é€»è¾‘æ£€æŸ¥ï¼š</strong> é™¤éå…è´¹ï¼Œå¦åˆ™ä»·æ ¼ä¸åº”ä¸ºé›¶'
    ),
    # Error Handling
    (
        r'Robust error handling sets professional scrapers apart from hobbyist scripts\. You need a strategy for different types of errors:',
        'ç¨³å¥çš„é”™è¯¯å¤„ç†å°†ä¸“ä¸šæŠ“å–å·¥å…·ä¸ä¸šä½™è„šæœ¬åŒºåˆ†å¼€æ¥ã€‚æ‚¨éœ€è¦åˆ¶å®šåº”å¯¹ä¸åŒç±»å‹é”™è¯¯çš„ç­–ç•¥ï¼š'
    ),
    (
        r'<strong>404 Not Found:</strong> The product might be delisted\. Log it and remove from queue\.',
        '<strong>404 æœªæ‰¾åˆ°ï¼š</strong> äº§å“å¯èƒ½å·²ä¸‹æ¶ã€‚è®°å½•å®ƒå¹¶ä»é˜Ÿåˆ—ä¸­ç§»é™¤ã€‚'
    ),
    (
        r'<strong>429 Too Many Requests:</strong> You are hitting limits\. Implement exponential backoff\.',
        '<strong>429 è¯·æ±‚è¿‡å¤šï¼š</strong> æ‚¨å·²è¾¾åˆ°é™åˆ¶ã€‚å®æ–½æŒ‡æ•°é€€é¿ç®—æ³•ã€‚'
    ),
    (
        r'<strong>5xx Server Errors:</strong> Amazon or API issue\. Retry after a delay\.',
        '<strong>5xx æœåŠ¡å™¨é”™è¯¯ï¼š</strong> Amazon æˆ– API é—®é¢˜ã€‚å»¶è¿Ÿåé‡è¯•ã€‚'
    ),
    # Rate Limiting
    (
        r'Even with an enterprise API, respecting limits is good citizenship and ensures stability\. Use the <code[^>]*>bucket token</code> algorithm or simple fixed-window counters to manage your request rate locally\.',
        'å³ä½¿ä½¿ç”¨ä¼ä¸šçº§ APIï¼Œéµå®ˆé™åˆ¶ä¹Ÿæ˜¯è‰¯å¥½çš„å…¬æ°‘è¡Œä¸ºå¹¶èƒ½ç¡®ä¿ç¨³å®šæ€§ã€‚ä½¿ç”¨<code class="text-accent-cyan">ä»¤ç‰Œæ¡¶</code>ç®—æ³•æˆ–ç®€å•çš„å›ºå®šçª—å£è®¡æ•°å™¨åœ¨æœ¬åœ°ç®¡ç†æ‚¨çš„è¯·æ±‚é€Ÿç‡ã€‚'
    ),
    # Performance Optimization
    (
        r'To get maximum throughput:',
        'ä¸ºäº†è·å¾—æœ€å¤§ååé‡ï¼š'
    ),
    (
        r'<strong>Keep Connections Alive:</strong> Use sessions to reuse TCP connections',
        '<strong>ä¿æŒè¿æ¥å­˜æ´»ï¼š</strong> ä½¿ç”¨ä¼šè¯ä»¥é‡ç”¨ TCP è¿æ¥'
    ),
    (
        r'<strong>Async I/O:</strong> Use asynchronous libraries like <code[^>]*>aiohttp</code> for non-blocking operations',
        '<strong>å¼‚æ­¥ I/Oï¼š</strong> å¯¹äºéé˜»å¡æ“ä½œï¼Œä½¿ç”¨åƒ <code class="text-accent-cyan">aiohttp</code> è¿™æ ·çš„å¼‚æ­¥åº“'
    ),
    (
        r'<strong>Minimal Headers:</strong> Send only necessary headers to reduce bandwidth',
        '<strong>æœ€å°æ ‡å¤´ï¼š</strong> ä»…å‘é€å¿…è¦çš„æ ‡å¤´ä»¥å‡å°‘å¸¦å®½'
    ),
    # Conclusion
    (
        r'Building a high-performance Amazon scraper requires more than just code; it requires architectural thinking\. By implementing these best practicesâ€”validation, error handling, and concurrencyâ€”you can build a system that is reliable and scalable\.',
        'æ„å»ºé«˜æ€§èƒ½ Amazon æŠ“å–å·¥å…·ä¸ä»…ä»…éœ€è¦ä»£ç ï¼›å®ƒéœ€è¦æ¶æ„æ€ç»´ã€‚é€šè¿‡å®æ–½è¿™äº›æœ€ä½³å®è·µâ€”â€”éªŒè¯ã€é”™è¯¯å¤„ç†å’Œå¹¶å‘â€”â€”æ‚¨å¯ä»¥æ„å»ºä¸€ä¸ªå¯é ä¸”å¯æ‰©å±•çš„ç³»ç»Ÿã€‚'
    ),
]

for pattern, replacement in translations:
    if '\\' not in pattern:
        content = content.replace(pattern.replace('\\', ''), replacement)
    else:
        content = re.sub(pattern, replacement, content)

print("âœ… ç¿»è¯‘å®Œæˆ")

with open(filename, 'w', encoding='utf-8') as f:
    f.write(content)
